{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3cc917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Character Text Splitting ####\n",
      "[Document(metadata={'source': 'local'}, page_content='Text splitting in LangChain is a cr'), Document(metadata={'source': 'local'}, page_content='itical feature that facilitates the'), Document(metadata={'source': 'local'}, page_content=' division of large texts into small'), Document(metadata={'source': 'local'}, page_content='er, manageable segments. ')]\n",
      "[Document(metadata={}, page_content='Text splitting in LangChain is a cr'), Document(metadata={}, page_content='itical feature that facilitates the'), Document(metadata={}, page_content=' division of large texts into small'), Document(metadata={}, page_content='er, manageable segments. ')]\n",
      "#### Recursive Character Text Splitting ####\n",
      "[Document(metadata={}, page_content='Input–Output and Interrupt\\n Example:'), Document(metadata={}, page_content='Fig. 1.12 Flowchart for interrupt cycle. Dr. Amarendra Kumar'), Document(metadata={}, page_content='Mishra VIPS-TC Dept. Electronics Engineering    Intro.. to COA.'), Document(metadata={}, page_content='• The branch instruction at address 1 causes the program'), Document(metadata={}, page_content='to transfer to the input–output service program at'), Document(metadata={}, page_content='address 1120.'), Document(metadata={}, page_content='• This program checks the flags, determines which flag is'), Document(metadata={}, page_content='set, and then transfers the required input or output'), Document(metadata={}, page_content='information. Once this is done, the instruction ION is'), Document(metadata={}, page_content='executed to set IEN to 1 (to enable further interrupts),'), Document(metadata={}, page_content='and the program returns to the location where it was'), Document(metadata={}, page_content='interrupted.'), Document(metadata={}, page_content='• The instruction that returns the computer to the original'), Document(metadata={}, page_content='place in the main program is a branch indirect'), Document(metadata={}, page_content='instruction with an address part of 0.'), Document(metadata={}, page_content='• This instruction is placed at the end of the I/O service'), Document(metadata={}, page_content='program.')]\n",
      "#### Document Specific Splitting ####\n",
      "[Document(metadata={}, page_content='# Fun in California\\n\\n## Driving'), Document(metadata={}, page_content='Try driving on the 1 down to San Diego'), Document(metadata={}, page_content='### Food'), Document(metadata={}, page_content=\"Make sure to eat a burrito while you're\"), Document(metadata={}, page_content='there'), Document(metadata={}, page_content='## Hiking\\n\\nGo to Yosemite')]\n",
      "[Document(metadata={}, page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'), Document(metadata={}, page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]\n",
      "[Document(metadata={}, page_content='// Function is called, the return value will end up in x'), Document(metadata={}, page_content='let x = myFunction(4, 3);'), Document(metadata={}, page_content='function myFunction(a, b) {'), Document(metadata={}, page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]\n",
      "#### Semantic Chunking ####\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    128\u001b[39m text_splitter = SemanticChunker(OpenAIEmbeddings())\n\u001b[32m    129\u001b[39m text_splitter = SemanticChunker(\n\u001b[32m    130\u001b[39m     OpenAIEmbeddings(), breakpoint_threshold_type=\u001b[33m\"\u001b[39m\u001b[33mpercentile\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# \"standard_deviation\", \"interquartile\"\u001b[39;00m\n\u001b[32m    131\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m documents = \u001b[43mtext_splitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(documents)\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# 5. Agentic Chunking\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:280\u001b[39m, in \u001b[36mSemanticChunker.create_documents\u001b[39m\u001b[34m(self, texts, metadatas)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[32m    279\u001b[39m     start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    281\u001b[39m         metadata = copy.deepcopy(_metadatas[i])\n\u001b[32m    282\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_start_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:228\u001b[39m, in \u001b[36mSemanticChunker.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.breakpoint_threshold_type == \u001b[33m\"\u001b[39m\u001b[33mgradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(single_sentences_list) == \u001b[32m2\u001b[39m\n\u001b[32m    226\u001b[39m ):\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m single_sentences_list\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m distances, sentences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_sentence_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_sentences_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.number_of_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    230\u001b[39m     breakpoint_distance_threshold = \u001b[38;5;28mself\u001b[39m._threshold_from_clusters(distances)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\langchain_experimental\\text_splitter.py:203\u001b[39m, in \u001b[36mSemanticChunker._calculate_sentence_distances\u001b[39m\u001b[34m(self, single_sentences_list)\u001b[39m\n\u001b[32m    199\u001b[39m _sentences = [\n\u001b[32m    200\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m\"\u001b[39m: x, \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: i} \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(single_sentences_list)\n\u001b[32m    201\u001b[39m ]\n\u001b[32m    202\u001b[39m sentences = combine_sentences(_sentences, \u001b[38;5;28mself\u001b[39m.buffer_size)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombined_sentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences):\n\u001b[32m    207\u001b[39m     sentence[\u001b[33m\"\u001b[39m\u001b[33mcombined_sentence_embedding\u001b[39m\u001b[33m\"\u001b[39m] = embeddings[i]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:591\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    590\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:479\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    483\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashpa\\OneDrive\\Desktop\\Chunking_In_RAG\\chun\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "local_llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# RAG\n",
    "def rag(chunks, collection_name):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        collection_name=collection_name,\n",
    "        embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | local_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = chain.invoke(\"What is the use of Text Splitting?\")\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# 1. Character Text Splitting\n",
    "print(\"#### Character Text Splitting ####\")\n",
    "\n",
    "text = \"Text splitting in LangChain is a critical feature that facilitates the division of large texts into smaller, manageable segments. \"\n",
    "\n",
    "# Manual Splitting\n",
    "chunks = []\n",
    "chunk_size = 35 # Characters\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
    "print(documents)\n",
    "\n",
    "# Automatic Text Splitting\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "documents = text_splitter.create_documents([text])\n",
    "print(documents)\n",
    "\n",
    "# 2. Recursive Character Text Splitting\n",
    "print(\"#### Recursive Character Text Splitting ####\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "with open('content.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0) # [\"\\n\\n\", \"\\n\", \" \", \"\"] 65,450\n",
    "print(text_splitter.create_documents([text])) \n",
    "\n",
    "# 3. Document Specific Splitting\n",
    "print(\"#### Document Specific Splitting ####\")\n",
    "\n",
    "# Document Specific Splitting - Markdown\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\"\n",
    "print(splitter.create_documents([markdown_text]))\n",
    "\n",
    "# Document Specific Splitting - Python\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\"\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "print(python_splitter.create_documents([python_text]))\n",
    "\n",
    "# Document Specific Splitting - Javascript\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "javascript_text = \"\"\"\n",
    "// Function is called, the return value will end up in x\n",
    "let x = myFunction(4, 3);\n",
    "\n",
    "function myFunction(a, b) {\n",
    "// Function returns the product of a and b\n",
    "  return a * b;\n",
    "}\n",
    "\"\"\"\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    ")\n",
    "print(js_splitter.create_documents([javascript_text]))\n",
    "\n",
    "# 4. Semantic Chunking\n",
    "print(\"#### Semantic Chunking ####\")\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Percentile - all differences between sentences are calculated, and then any difference greater than the X percentile is split\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
    ")\n",
    "documents = text_splitter.create_documents([text])\n",
    "print(documents)\n",
    "\n",
    "# 5. Agentic Chunking\n",
    "print(\"#### Proposition-Based Chunking ####\")\n",
    "\n",
    "# https://arxiv.org/pdf/2312.06648.pdf\n",
    "\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import create_extraction_chain\n",
    "from typing import Optional, List\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain import hub\n",
    "\n",
    "obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "runnable = obj | llm\n",
    "\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "    \n",
    "# Extraction\n",
    "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n",
    "def get_propositions(text):\n",
    "    runnable_output = runnable.invoke({\n",
    "    \t\"input\": text\n",
    "    }).content\n",
    "    propositions = extraction_chain.invoke(runnable_output)[\"text\"][0].sentences\n",
    "    return propositions\n",
    "    \n",
    "paragraphs = text.split(\"\\n\\n\")\n",
    "text_propositions = []\n",
    "for i, para in enumerate(paragraphs[:5]):\n",
    "    propositions = get_propositions(para)\n",
    "    text_propositions.extend(propositions)\n",
    "    print (f\"Done with {i}\")\n",
    "\n",
    "print (f\"You have {len(text_propositions)} propositions\")\n",
    "print(text_propositions[:10])\n",
    "\n",
    "print(\"#### Agentic Chunking ####\")\n",
    "\n",
    "from agentic_chunker import AgenticChunker\n",
    "ac = AgenticChunker()\n",
    "ac.add_propositions(text_propositions)\n",
    "print(ac.pretty_print_chunks())\n",
    "chunks = ac.get_chunks(get_type='list_of_strings')\n",
    "print(chunks)\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
    "rag(documents, \"agentic-chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e89ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
