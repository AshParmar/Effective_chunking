{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6ed2a8",
   "metadata": {},
   "source": [
    "# Agentic Chunking with Groq (Free Alternative)\n",
    "\n",
    "This notebook implements agentic chunking using Groq's free API instead of OpenAI.\n",
    "\n",
    "## Setup Instructions:\n",
    "\n",
    "1. **Get a free Groq API key:**\n",
    "   - Go to https://console.groq.com/\n",
    "   - Sign up for a free account\n",
    "   - Create an API key from the dashboard\n",
    "\n",
    "2. **Set up your API key:**\n",
    "   - Create a `.env` file in your project directory\n",
    "   - Add: `GROQ_API_KEY=your_api_key_here`\n",
    "   - Or set it as an environment variable\n",
    "\n",
    "3. **Install required packages:**\n",
    "   Run the cell below to install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain-groq python-dotenv rich pydantic langchain-core langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602518ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The month is October.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'The month is October.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks, creating a new one\n",
       "</pre>\n"
      ],
      "text/plain": [
       "No chunks, creating a new one\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>5398e<span style=\"font-weight: bold\">)</span>: Date &amp; Times\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Created new chunk \u001b[1m(\u001b[0m5398e\u001b[1m)\u001b[0m: Date & Times\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The year is 2023.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'The year is 2023.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashpa\\AppData\\Local\\Temp\\ipykernel_25796\\2746312798.py:297: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  extraction_chain = create_extraction_chain_pydantic(pydantic_schema=ChunkID, llm=self.llm)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>5398e<span style=\"font-weight: bold\">)</span>, adding to: Date &amp; Times\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0m5398e\u001b[1m)\u001b[0m, adding to: Date & Times\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'One of the most important things that I didn'</span>t understand about the world as a child was the degree to \n",
       "which the returns for performance are superlinear.'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'One of the most important things that I didn'\u001b[0mt understand about the world as a child was the degree to \n",
       "which the returns for performance are superlinear.'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>5398e<span style=\"font-weight: bold\">)</span>, adding to: Date &amp; Times\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0m5398e\u001b[1m)\u001b[0m, adding to: Date & Times\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'Teachers and coaches implicitly told us that the returns were linear.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'Teachers and coaches implicitly told us that the returns were linear.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>5398e<span style=\"font-weight: bold\">)</span>, adding to: Performance and Returns\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0m5398e\u001b[1m)\u001b[0m, adding to: Performance and Returns\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'I heard a thousand times that '</span>You get out what you put in.<span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding: \u001b[32m'I heard a thousand times that '\u001b[0mYou get out what you put in.\u001b[32m''\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>5398e<span style=\"font-weight: bold\">)</span>, adding to: Date &amp; Times\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Found \u001b[1m(\u001b[0m5398e\u001b[1m)\u001b[0m, adding to: Date & Times\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "You have <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> chunks\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "You have \u001b[1;36m1\u001b[0m chunks\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk #\u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: 5398e\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk ID: 5398e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about dates and times, performance, and the relationship between effort \n",
       "and outcome.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Summary: This chunk contains information about dates and times, performance, and the relationship between effort \n",
       "and outcome.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Propositions:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The month is October.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    -The month is October.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The year is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    -The year is \u001b[1;36m2023\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -One of the most important things that I didn't understand about the world as a child was the degree to which \n",
       "the returns for performance are superlinear.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    -One of the most important things that I didn't understand about the world as a child was the degree to which \n",
       "the returns for performance are superlinear.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -Teachers and coaches implicitly told us that the returns were linear.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    -Teachers and coaches implicitly told us that the returns were linear.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -I heard a thousand times that <span style=\"color: #008000; text-decoration-color: #008000\">'You get out what you put in.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    -I heard a thousand times that \u001b[32m'You get out what you put in.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Outline\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk Outline\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk <span style=\"font-weight: bold\">(</span>5398e<span style=\"font-weight: bold\">)</span>: Effort and Outcome\n",
       "Summary: This chunk contains information about dates and times, performance, and the relationship between effort \n",
       "and outcome.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chunk \u001b[1m(\u001b[0m5398e\u001b[1m)\u001b[0m: Effort and Outcome\n",
       "Summary: This chunk contains information about dates and times, performance, and the relationship between effort \n",
       "and outcome.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The month is October. The year is 2023. One of the most important things that I didn't understand about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world as a child was the degree to which the returns for performance are superlinear. Teachers and coaches </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implicitly told us that the returns were linear. I heard a thousand times that 'You get out what you put in.'\"</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"The month is October. The year is 2023. One of the most important things that I didn't understand about the \u001b[0m\n",
       "\u001b[32mworld as a child was the degree to which the returns for performance are superlinear. Teachers and coaches \u001b[0m\n",
       "\u001b[32mimplicitly told us that the returns were linear. I heard a thousand times that 'You get out what you put in.'\"\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import uuid\n",
    "from langchain_groq import ChatGroq  # Changed from langchain_openai to langchain_groq\n",
    "import os\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AgenticChunker:\n",
    "    def __init__(self, groq_api_key=None):  # Changed parameter name\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "\n",
    "        # Whether or not to update/refine summaries and titles as you get new information\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "\n",
    "        if groq_api_key is None:\n",
    "            groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "        if groq_api_key is None:\n",
    "            raise ValueError(\"API key is not provided and not found in environment variables\")\n",
    "\n",
    "        # Fixed: Use ChatGroq instead of ChatOpenAI, and correct model name\n",
    "        self.llm = ChatGroq(model='llama-3.1-8b-instant', api_key=groq_api_key, temperature=0)\n",
    "\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "    \n",
    "    def add_proposition(self, proposition):\n",
    "        if self.print_logging:\n",
    "            print (f\"\\nAdding: '{proposition}'\")\n",
    "\n",
    "        # If it's your first chunk, just make a new chunk and don't check for others\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "\n",
    "        # If a chunk was found then add the proposition to it\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "            return\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks found\")\n",
    "            # If a chunk wasn't found, then create a new one\n",
    "            self._create_new_chunk(proposition)\n",
    "        \n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
    "        # Add then\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "\n",
    "        # Then grab a new summary\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the chunk new summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary']\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _update_chunk_title(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good title will say what the chunk is about.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
    "\n",
    "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        updated_chunk_title = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary'],\n",
    "            \"current_title\" : chunk['title']\n",
    "        }).content\n",
    "\n",
    "        return updated_chunk_title\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the new chunk summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": proposition\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _get_new_chunk_title(self, summary):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "                    You will be given a summary of a chunk which needs a title\n",
    "\n",
    "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_title = runnable.invoke({\n",
    "            \"summary\": summary\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_title\n",
    "\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id' : new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title' : new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index' : len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "    \n",
    "    def get_chunk_outline(self):\n",
    "        \"\"\"\n",
    "        Get a string which represents the chunks you currently have.\n",
    "        This will be empty when you first start off\n",
    "        \"\"\"\n",
    "        chunk_outline = \"\"\n",
    "\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ({chunk['chunk_id']}): {chunk['title']}\\nSummary: {chunk['summary']}\\n\\n\"\"\"\n",
    "        \n",
    "            chunk_outline += single_chunk_string\n",
    "        \n",
    "        return chunk_outline\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
    "\n",
    "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
    "                    The goal is to group similar propositions and chunks.\n",
    "\n",
    "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
    "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
    "\n",
    "                    Example:\n",
    "                    Input:\n",
    "                        - Proposition: \"Greg really likes hamburgers\"\n",
    "                        - Current Chunks:\n",
    "                            - Chunk ID: 2n4l3d\n",
    "                            - Chunk Name: Places in San Francisco\n",
    "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
    "\n",
    "                            - Chunk ID: 93833k\n",
    "                            - Chunk Name: Food Greg likes\n",
    "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
    "                    Output: 93833k\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
    "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        chunk_found = runnable.invoke({\n",
    "            \"proposition\": proposition,\n",
    "            \"current_chunk_outline\": current_chunk_outline\n",
    "        }).content\n",
    "\n",
    "        # Pydantic data class\n",
    "        class ChunkID(BaseModel):\n",
    "            \"\"\"Extracting the chunk id\"\"\"\n",
    "            chunk_id: Optional[str]\n",
    "            \n",
    "        # Extraction to catch-all LLM responses. This is a bandaid\n",
    "        extraction_chain = create_extraction_chain_pydantic(pydantic_schema=ChunkID, llm=self.llm)\n",
    "        extraction_found = extraction_chain.invoke(chunk_found)[\"text\"]\n",
    "        if extraction_found:\n",
    "            chunk_found = extraction_found[0].chunk_id\n",
    "\n",
    "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
    "        # So return nothing\n",
    "        if len(chunk_found) != self.id_truncate_limit:\n",
    "            return None\n",
    "\n",
    "        return chunk_found\n",
    "    \n",
    "    def get_chunks(self, get_type='dict'):\n",
    "        \"\"\"\n",
    "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
    "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
    "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
    "        \"\"\"\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            chunks = []\n",
    "            for chunk_id, chunk in self.chunks.items():\n",
    "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
    "            return chunks\n",
    "    \n",
    "    def pretty_print_chunks(self):\n",
    "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    -{prop}\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def pretty_print_chunk_outline(self):\n",
    "        print (\"Chunk Outline\\n\")\n",
    "        print(self.get_chunk_outline())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ac = AgenticChunker()\n",
    "\n",
    "    ## Comment and uncomment the propositions to your hearts content\n",
    "    propositions = [\n",
    "        'The month is October.',\n",
    "        'The year is 2023.',\n",
    "        \"One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\",\n",
    "        'Teachers and coaches implicitly told us that the returns were linear.',\n",
    "        \"I heard a thousand times that 'You get out what you put in.'\",\n",
    "        # 'Teachers and coaches meant well.',\n",
    "        # \"The statement that 'You get out what you put in' is rarely true.\",\n",
    "        # \"If your product is only half as good as your competitor's product, you do not get half as many customers.\",\n",
    "        # \"You get no customers if your product is only half as good as your competitor's product.\",\n",
    "        # 'You go out of business if you get no customers.',\n",
    "        # 'The returns for performance are superlinear in business.',\n",
    "        # 'Some people think the superlinear returns for performance are a flaw of capitalism.',\n",
    "        # 'Some people think that changing the rules of capitalism would stop the superlinear returns for performance from being true.',\n",
    "        # 'Superlinear returns for performance are a feature of the world.',\n",
    "        # 'Superlinear returns for performance are not an artifact of rules that humans have invented.',\n",
    "        # 'The same pattern of superlinear returns is observed in fame.',\n",
    "        # 'The same pattern of superlinear returns is observed in power.',\n",
    "        # 'The same pattern of superlinear returns is observed in military victories.',\n",
    "        # 'The same pattern of superlinear returns is observed in knowledge.',\n",
    "        # 'The same pattern of superlinear returns is observed in benefit to humanity.',\n",
    "        # 'In fame, power, military victories, knowledge, and benefit to humanity, the rich get richer.'\n",
    "    ]\n",
    "    \n",
    "    ac.add_propositions(propositions)\n",
    "    ac.pretty_print_chunks()\n",
    "    ac.pretty_print_chunk_outline()\n",
    "    print (ac.get_chunks(get_type='list_of_strings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1299bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
